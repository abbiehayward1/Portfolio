{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Investigating spark with PySpark\n",
        "\n",
        "Implement a simple mapping-and-reducing problem, code provided as appenidx, tutotrial write up in Portfolio Section 10.\n",
        "\n",
        "References:\n",
        "* PySpark tutorial found on youtube to familiarise and perform simple operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEvyDJwVab-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often used for large scale data processing and machine learning. Using Apache spark runs workloads 100x faster.\n",
        "\n"
      ],
      "metadata": {
        "id": "GHmiq2wKakYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Installation of pyspark in google colab found at:\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bya4tAMpbFko",
        "outputId": "1500b1f3-3e5a-4ad0-e8ac-d0f6107e0381"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "44 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "PNJ95-26kKbm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to create a spark session"
      ],
      "metadata": {
        "id": "jxomeAqklQhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('Practice').getOrCreate()"
      ],
      "metadata": {
        "id": "akykyGYtkk0w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "eHCGOjcQkl35",
        "outputId": "d5f34135-50c8-464a-8f71-f60a7c604b85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aecaf305480>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://14d91b27e384:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Practice</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying to implement a simple mapping and reducing problem (similar to that in the workshop)"
      ],
      "metadata": {
        "id": "TFR1tkGNvq70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapping with Pyspark"
      ],
      "metadata": {
        "id": "Dp2p_sRuxP66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** to perform mapping in pySpark, could use a UDF (user defined function ) that works element-wise on a single column, or Row-wise RDD (Resilient distributed dataset) operations, that allows adding functions across multiple columns in a dataframe.\n",
        "\n",
        " To truly utilise the powers of Pyspark, I will be using a RDD (showing quicker mapping functions)!!!"
      ],
      "metadata": {
        "id": "joysCnPyEdfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a spark session\n",
        "from pyspark import SparkContext\n",
        "#sc = SparkContext(\"local\", \"Mapping\")\n",
        "data = [1,2,3,4]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "#simple mapping of element to their square\n",
        "# a.map(lambda x: x+1)\n",
        "#a : dataframe\n",
        "# map : map transformation to be applied\n",
        "# lambda: the funcion to be applied\n",
        "#rdd = sc.parallelize([1,2,3,4])\n",
        "squared_rdd = rdd.map(lambda x: x**2)\n",
        "squared_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWKUzs0nmWat",
        "outputId": "a2085b98-7895-4746-dc0c-cfb11bcc91d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above utilised the 'map' function, that is a transformation in PySPark that is applied over each and every function/instance of an RDD/DataFrame in the spark. It returns a new RDD r dataframe where the MAp function is applied.\n",
        "\n",
        "The transformation iterates over elements and applies the logic to each element  returning a new RDD/DataFrame\n",
        "\n",
        "First you need to create a Pyspark RDD: this can be done with a = sc.parallelize\n",
        "\n",
        "To get our results call b.collect() where b is what our new RDD is after mapping has been applied `b = a.map(lambda x: x +1)'"
      ],
      "metadata": {
        "id": "MPkwg1XKncCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combining elementwise\n",
        "rdd1 = spark.sparkContext.parallelize(['A', 'B', 'C'])\n",
        "rdd2 = spark.sparkContext.parallelize([1,2,3])\n",
        "combined_rdd = rdd1.zip(rdd2).map(lambda x: (x[0], x[1]))\n",
        "combined_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-6NiqBYmrGe",
        "outputId": "756e5a77-dc1a-4d90-de6c-0145826b1a15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 1), ('B', 2), ('C', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing with Pyspark"
      ],
      "metadata": {
        "id": "2ezkdLYKvtbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit"
      ],
      "metadata": {
        "id": "WNcSbb0kvtU_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reduce funciton requires two arguments. The first is the function we want to repeat (for example addition, summing up columns), and the second is an iterable that we can to repeat over (df, columns, etc).\n",
        "\n",
        "https://towardsdatascience.com/reduce-your-worries-using-reduce-with-pyspark-642106d6ae50"
      ],
      "metadata": {
        "id": "p8FAmr9hyV96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will be over the form `reduce(lambda x, y: x + y, [1,2,3,4,5])"
      ],
      "metadata": {
        "id": "OPlfP_2ot9oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple example\n",
        "from operator import add\n",
        "sum = rdd.reduce(add)"
      ],
      "metadata": {
        "id": "DZZFVhxDyV1u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce can be used to find a maximum element too...\n",
        "max_square = squared_rdd.reduce(lambda x, y : x if x > y else y)\n",
        "print(max_square)\n",
        "\n",
        "sum_rdd2 = rdd2.reduce(lambda x, y : x+ y)\n",
        "print(sum_rdd2)"
      ],
      "metadata": {
        "id": "ouPOCCtFyVtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39a99d6-9d08-4655-96db-ca804f03d1a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "QL-Cu2ZbyVgQ"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}